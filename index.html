<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yongjie Zhu</title>
  
  <meta name="author" content="Yongjie Zhu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/ai_icon.png">
  <script src="script/functions.js"></script> 
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yongjie Zhu</name>
              </p>
              <p>I am currently a Researcher at <a href="https://kling.kuaishou.com/en">Kling Team</a>, <a href="https://www.kuaishou.com/en">Kuaishou Technology</a> (Beijing). I obtained my Master degree from School of Artificial Intelligence, <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications (BUPT)</a>, co-supervised by Prof. <a href="https://www.pris.net.cn/introduction/teacher/lisi">Si Li</a> and Prof. <a href="https://camera.pku.edu.cn/">Boxin Shi</a>. 
              <!-- <p>I am a third year Master student in the School of Artificial Intelligence at <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications (BUPT)</a>, supervised by Prof. <a href="https://www.pris.net.cn/introduction/teacher/lisi">Si Li</a> and Prof. <a href="http://alumni.media.mit.edu/~shiboxin/index.html">Boxin Shi</a>.  -->
              </p>
              <p>
                Previously, I was a research intern at <a href="https://en.wikipedia.org/wiki/Microsoft">Microsoft</a>, where I work on computer vision and natural language processing. I also worked as a research intern at Tencent, WXG. At Tencent, I've worked on <a href="">3D Human Body Estimation</a> and <a href="">Face Appearance Modeling</a> under the guidance of Dr. <a href="https://www.lichenpro.com/">Chen Li</a>. 
              </p>
              <p>I am looking for self-motivated interns for mulimodal understanding and generation research! If you are interested in <b>internship at Kling or collaboration of research</b>, please email me.
              </p>
              <p style="text-align:center">
                <a href="mailto:yongjie.zhu.96@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/YongjieZhu-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/YongjieZhu-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=AQLfp6cAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yongjie-zhu-%E6%9C%B1%E5%8B%87%E6%9D%B0-38a145156/">LinkedIn</a> &nbsp &nbsp
                <!-- <a href="https://github.com/Yongjie-Zhu/">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/YongjieZhu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/YongjieZhu_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <heading>News</heading>
          <ul>
            <li><strong>[2025.12]</strong> Kling-Omni Technical Report is released. <a href="https://arxiv.org/abs/2512.16776"> [Link]</a>. </li> 
            <li><strong>[2025.09]</strong> One paper accepted to <a href="https://neurips.cc/"> NeurIPS 2025</a>. </li> 
            <li><strong>[2025.05]</strong> One <span style="color:#ff0000;"><strong>spotlight</strong></span> paper accepted to <a href="">ICML 2025</a>. </li>
           
            <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
            <div id="old_news" style="display: none;">
              <li><strong>[2023.10]</strong> One paper accepted to <a href="">TPAMI</a>. </li>  
              <li><strong>[2023.02]</strong> One paper accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>. </li> 
              <li><strong>[2022.11]</strong> Excellent graduation thesis of BUPT. </li>  
              <li><strong>[2022.07]</strong> One <span style="color:#ff0000;"><strong>oral</strong></span> paper accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>. </li> 
              <li><strong>[2022.06]</strong> <a href="">AdsCVLR</a> is accepted by <a href="https://2022.acmmm.org/">ACM MM 2022</a>. </li>  
              <li><strong>[2021.11]</strong> Received China National Scholarship. </li>
              <li><strong>[2021.05]</strong> <a href="http://cvpr2021.thecvf.com/node/184?fbclid=IwAR3zy5xnT4xm3v5uaF8DpUuOadKC0MsgM9sWE39iDivZoSaq0UtgMARFaiA">Outstanding reviewer</a> of <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>. </li>
              <li><strong>[2021.05]</strong> The extension of my bachelor thesis <a href="">HyFRIS-Net</a> got accepted by <a href="">TPAMI</a>! </li> 
               <li><strong>[2021.03]</strong>  <a href="https://arxiv.org/abs/2104.04160">SOLID-Net</a> is accepted by <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a> as <span style="color:#ff0000;"><strong>oral presentation</strong></span> <strong>(top 4%)</strong>!</li>
              <li><strong>[2021.03]</strong> <a href="https://arxiv.org/abs/2104.13602">DeRenderNet</a> is accepted by <a href="https://iccp-conference.org/">ICCP 2021</a>. </li>
            </div></div>
          </ul>
        </td>
      </tr>
      <!-- <tr> -->
      <!--   <td width="100%" valign="middle"> -->
      <!--     <heading>Research</heading> -->
      <!--   </td> -->
      <!-- </tr> -->
    </table> 
    
        <!-- <\!-- <hr class="split"><br/> -\-> -->
        
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
        <!--     <tr> -->
        <!--     <td style="padding:20px;width:100%;vertical-align:middle"> -->
        <!--       <heading>News</heading> -->
        <!--       <p> -->
        <!--                         &nbsp;&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp; [2021.11] Received China National Scholarship. -->
        <!--         <Br> -->
        <!--         &nbsp;&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp; [2021.05] <a href="http://cvpr2021.thecvf.com/node/184?fbclid=IwAR3zy5xnT4xm3v5uaF8DpUuOadKC0MsgM9sWE39iDivZoSaq0UtgMARFaiA">Outstanding reviewer</a> of <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>. -->
        <!--         <br> -->
        <!--         &nbsp;&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp; [2021.05] <a href="">HyFRIS-Net</a> accepted to <a href="">TPAMI 2021</a>. -->
        <!--         <br> -->
        <!--         &nbsp;&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp; [2021.03] <a href="https://arxiv.org/abs/2104.04160">SOLID-Net</a> accepted to <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a> Oral presentation. -->
        <!--         <br> -->
        <!--         &nbsp;&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp; [2021.03] <a href="https://arxiv.org/abs/2104.13602">DeRenderNet</a> accepted to <a href="https://iccp-conference.org/">ICCP 2021</a>. -->
        <!--       </p> -->
        <!--     </td> -->
        <!--   </tr> -->
        <!-- </tbody></table> -->
 
        <!-- <\!-- <hr class="split"><br/> -\-> -->
               
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <br>
              (See full list at  <a href="https://scholar.google.com/citations?user=AQLfp6cAAAAJ&hl=en">Google Scholar</a>, * indicates equal contribution, † indicates project lead.)
              <p>
                My research interests lie in multimodal understanding, with an emphasis on integrating vision, audio, language, and physical cues for grounded reasoning. Recently, I have focused on physics-aware and generative approaches to help models understand the physical structure and dynamics of the world, enabling richer multimodal perception and reasoning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr onmouseout="eccv26_stop()" onmouseover="eccv26_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='eccv26_image'>
                  <img src='images/eccv26_before.png' width="160"></div>
                <img src='images/eccv26_after.png' width="160">
              </div>
              <script type="text/javascript">
                function eccv26_start() {
                  document.getElementById('eccv26_image').style.opacity = "1";
                }

                function eccv26_stop() {
                  document.getElementById('eccv26_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings</papertitle>
              </a>
              <br>
              <a href="">Haonan Jiang*</a>,
              <a href="">Yuji Wang*</a>, 
              <strong>Yongjie Zhu†</strong>,
              <a href="">Xin Lu</a>, 
              <a href="">Wenyu Qin</a>,
              <a href="">Meng Wang</a>,
              <a href="">Pengfei Wan</a>,
              <a href="">Yansong Tang</a>
              <br>
              <em>Arxiv</em>, 2026 &nbsp;
 
              <br>
              <a href="https://arxiv.org/abs/2602.13823">[paper]</a>   &nbsp;               
              <a href="https://github.com/ZoengHN/Embed-RL">[project]</a>  &nbsp; 
              <!-- <a href="https://github.com/ChemJeff/SOLD-Net/">[project]</a>  -->
              <p>
              We propose a reasoning-driven Universal Multimodal Embedding framework that leverages Embedder-Guided Reinforcement Learning to generate retrieval-aligned, evidential Traceability CoTs, significantly improving cross-modal semantic consistency, fine-grained matching, and benchmark performance under limited computational resources.
              </p>
            </td>
          </tr> 

            
          <tr onmouseout="icml26_stop()" onmouseover="icml26_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icml26_image'>
                  <img src='images/icml26_b.png' width="160"></div>
                <img src='images/icml26_a.png' width="160">
              </div>
              <script type="text/javascript">
                function icml26_start() {
                  document.getElementById('icml26_image').style.opacity = "1";
                }

                function icml26_stop() {
                  document.getElementById('icml26_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Analytic Score Optimization for Multi Dimension Video Quality Assessment</papertitle>
              </a>
              <br>
              <a href="">Boda Lin</a>,              
              <strong>Yongjie Zhu†</strong>,
              <a href="">Wenyu Qin</a>,
              <a href="">Meng Wang</a>,
              <a href="">Pengfei Wan</a>
              <br>
              <em>Arxiv</em>, 2026 &nbsp;
 
              <br>
              <a href="https://www.arxiv.org/abs/2602.16856">[paper]</a>   &nbsp;               
              <a href="">[project]</a>  &nbsp; 
              <!-- <a href="https://github.com/ChemJeff/SOLD-Net/">[project]</a>  -->
              <p>
               We introduce UltraVQA, a large-scale multi-dimensional video quality assessment dataset with rich human and GPT-augmented annotations, and propose Analytic Score Optimization (ASO), a theoretically grounded post-training objective that models quality assessment as a regularized decision process, achieving superior performance and lower MAE compared to existing closed- and open-source baselines. 
              </p>
            </td>
          </tr> 


            
          <tr onmouseout="cvpr26_stop()" onmouseover="cvpr26_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr26_image'>
                  <img src='images/cvpr26_before_1.png' width="160"></div>
                <img src='images/cvpr26_after.png' width="160">
              </div>
              <script type="text/javascript">
                function cvpr26_start() {
                  document.getElementById('cvpr26_image').style.opacity = "1";
                }

                function cvpr26_stop() {
                  document.getElementById('cvpr26_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Beyond the Golden Data: Resolving the Motion-Vision Quality Dilemma via Timestep Selective Training</papertitle>
              </a>
              <br>
              <a href="">Xiangyang Luo</a>,
              <a href="">Qingyu Li</a>,
              <a href="">Yuming Li</a>,
              <a href="">Guanbo Huang</a>, 
              <strong>Yongjie Zhu</strong>,
              <a href="">Wenyu Qin</a>,
              <a href="">Meng Wang</a>, 
              <a href="">Pengfei Wan</a>,              
              <a href="">Shao-Lun Huang</a> 
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2026 &nbsp;
 
              <br>
              <a href="">[paper]</a>   &nbsp;               
              <a href="">[project]</a>  &nbsp; 
              <!-- <a href="https://github.com/ChemJeff/SOLD-Net/">[project]</a>  -->
              <p>
               We identify a negative correlation between visual and motion quality in video data and propose Timestep-aware Quality Decoupling (TQD), which leverages timestep-specific sampling to decouple imbalanced data and achieve performance surpassing conventional training with even better data. 
              </p>
            </td>
          </tr> 

            
          <tr onmouseout="nips25_stop()" onmouseover="nips25_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nips25_image'>
                  <img src='images/nips25_before_1.png' width="160"></div>
                <img src='images/nips25_after_1.png' width="160">
              </div>
              <script type="text/javascript">
                function nips25_start() {
                  document.getElementById('nips25_image').style.opacity = "1";
                }

                function nips25_stop() {
                  document.getElementById('nips25_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models</papertitle>
              </a>
              <br>
              <a href="">Zhicheng Zhang*</a>,
              <a href="">Weicheng Wang*</a>,                                        
              <strong>Yongjie Zhu†</strong>,
              <a href="">Wenyu Qin</a>,
              <a href="">Pengfei Wan</a>,
              <a href="">Di Zhang</a>,
              <a href="">Jufeng Yang</a> 
              <br>
              <em>The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)</em>, 2025 &nbsp;
 
              <br>
              <a href="https://arxiv.org/abs/2511.02712v1">[paper]</a>   &nbsp;               
              <a href="https://zzcheng.top/VidEmo/">[project]</a>  &nbsp; 
              <!-- <a href="https://github.com/ChemJeff/SOLD-Net/">[project]</a>  -->
              <p>
               We introduce the VidEmo framework and the Emo-CFG dataset, enabling affective cue–guided video emotion understanding and significantly outperforming existing VideoLLMs across multiple video perception tasks. 
              </p>
            </td>
          </tr> 
            
            
          <tr onmouseout="icml25_stop()" onmouseover="icml25_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icml25_image'>
                  <img src='images/icml25_before_2.png' width="160"></div>
                <img src='images/icml25_after_1.png' width="160">
              </div>
              <script type="text/javascript">
                function icml25_start() {
                  document.getElementById('icml25_image').style.opacity = "1";
                }

                function icml25_stop() {
                  document.getElementById('icml25_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding</papertitle>
              </a>
              <br>
              <a href="">Zhicheng Zhang</a>,
              <a href="">Wuyou Xia</a>,
              <a href="">Chenxi Zhao</a>,
              <a href="">Yan Zhou</a>,
              <a href="">Xiaoqiang Liu</a>,
              <strong>Yongjie Zhu†</strong>,
              <a href="">Wenyu Qin</a>,
              <a href="">Pengfei Wan</a>,
              <a href="">Di Zhang</a>,
              <a href="">Jufeng Yang</a> 
              <br>
              <em>Forty-Second International Conference on Machine Learning (ICML)</em>, 2025 &nbsp; <font color="red"><b>(Spotlight Poster)</b></font>
 
              <br>
              <a href="https://arxiv.org/abs/2507.04635">[paper]</a>   &nbsp;               
              <a href="https://zzcheng.top/MODA/">[project]</a>  &nbsp; 
              <!-- <a href="https://github.com/ChemJeff/SOLD-Net/">[project]</a>  -->
              <p>
                A modular attention-empowered Multimodal LLM that delving deep into the fine-grained cues for emotion understanding and cognition analysis.
              </p>
            </td>
          </tr> 



            <tr onmouseout="tpami23_stop()" onmouseover="tpami23_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tpami23_image'>
                  <img src='images/tpami23_before_1.png' width="160"></div>
                <img src='images/tpami23_after_1.png' width="160">
              </div>
              <script type="text/javascript">
                function tpami23_start() {
                  document.getElementById('tpami23_image').style.opacity = "1";
                }

                function tpami23_stop() {
                  document.getElementById('tpami23_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>SPLiT: Single Portrait Lighting Estimation via a Tetrad of Face Intrinsics</papertitle>
              </a>
              <br>
              <a href="">Fan Fei*</a>,
              <a href="">Yean Cheng*</a>,
              <strong>Yongjie Zhu</strong>,
              <a href="">Qian Zheng</a>,
              <a href="">Si Li</a>,
              <a href="">Gang Pan</a>, 
              <a href="http://alumni.media.mit.edu/~shiboxin/index.html">Boxin Shi</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2023  
 
              <br>
              <a href="https://www.computer.org/csdl/journal/tp/2024/02/10301699/1RFBJBi1aHS">[paper]</a>   &nbsp;               
              <a href="https://costrice.github.io/split/">[project]</a>  &nbsp; 
              <!-- <a href="https://github.com/ChemJeff/SOLD-Net/">[project]</a>  -->
              <p>
                This paper proposes a novel pipeline to estimate a non-parametric environment map with high dynamic range from a single human face image.
              </p>
            </td>
          </tr> 
            
 
            <tr onmouseout="cvpr23_stop()" onmouseover="cvpr23_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr23_image'>
                  <img src='images/cvpr23_after_2.png' width="160"></div>
                <img src='images/cvpr23_before_2.png' width="160">
              </div>
              <script type="text/javascript">
                function cvpr23_start() {
                  document.getElementById('cvpr23_image').style.opacity = "1";
                }

                function cvpr23_stop() {
                  document.getElementById('cvpr23_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Complementary intrinsics from neural radiance fields and CNNs for outdoor scene relighting</papertitle>
              </a>
              <br>
              <a href="">Siqi Yang*</a>,
              <a href="">Xuanning Cui*</a>,
              <strong>Yongjie Zhu</strong>,
              <a href="">Jiajun Tang</a>,
              <a href="">Si Li</a>,
              <a href="">Zhaofei Yu</a>, 
              <a href="http://alumni.media.mit.edu/~shiboxin/index.html">Boxin Shi</a>
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023  
 
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Complementary_Intrinsics_From_Neural_Radiance_Fields_and_CNNs_for_Outdoor_CVPR_2023_paper.pdf">[paper]</a>   &nbsp; 
              <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Yang_Complementary_Intrinsics_From_CVPR_2023_supplemental.pdf">[supp]</a>  &nbsp;  
              <a href="data/Yang_CVPR_2023.bib">[bibtex]</a>  &nbsp; 
              <!-- <a href="https://github.com/ChemJeff/SOLD-Net/">[project]</a>  -->
              <p>
                This paper proposes to complement the intrinsic estimation from volume rendering using NeRF and from inversing the photometric image formation model using convolutional neural networks (CNNs).
              </p>
            </td>
          </tr> 
 
            
          <tr onmouseout="eccv22_stop()" onmouseover="eccv22_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='eccv22_image'>
                  <img src='images/eccv22_after_1.png' width="160"></div>
                <img src='images/eccv22_before_1.png' width="160">
              </div>
              <script type="text/javascript">
                function eccv22_start() {
                  document.getElementById('eccv22_image').style.opacity = "1";
                }

                function eccv22_stop() {
                  document.getElementById('eccv22_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Estimating Spatially-Varying Lighting in Urban Scenes with Disentangled Representation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=UBy9z8wAAAAJ&hl=zh-CN">Jiajun Tang</a>, 
              <strong>Yongjie Zhu</strong>,
              <a href="">Haoyu Wang</a>,
              <a href="">Jun Hoong Chan</a>,
              <a href="">Si Li</a>,
              <a href="http://alumni.media.mit.edu/~shiboxin/index.html">Boxin Shi</a>
              <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2022  &nbsp; <font color="red"><b>(Oral Presentation)</b></font> 
 
              <br>
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660445.pdf">[paper]</a>   &nbsp; 
              <a href="data/Tang_ECCV_2022.bib">[bibtex]</a>  &nbsp; 
              <a href="https://github.com/ChemJeff/SOLD-Net/">[project]</a> 
              <p>
                Given a single image and a 2D pixel location, our method can estimate the local lighting that is disentangled into ambient sky light, sun light and lighting-independent local contents.
              </p>
            </td>
          </tr> 

            
          <tr onmouseout="adscvlr_stop()" onmouseover="adscvlr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='adscvlr_image'>
                  <img src='images/adscvlr_after.png' width="160"></div>
                <img src='images/adscvlr_before.png' width="160">
              </div>
              <script type="text/javascript">
                function adscvlr_start() {
                  document.getElementById('adscvlr_image').style.opacity = "1";
                }

                function adscvlr_stop() {
                  document.getElementById('adscvlr_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>AdsCVLR: Commercial Visual-Linguistic Representation Modeling in Sponsored Search</papertitle>
              </a>
              <br>
              <strong>Yongjie Zhu</strong>,
              <a href="">Chunhui Han</a>,
              <a href="">Yuefeng Zhan</a>,
              <a href="">Bochen Pang</a>,
              <a href="">Zhaoju Li</a>,
              <a href="">Hao Sun</a>,
              <a href="">Si Li</a>,
              <a href="http://alumni.media.mit.edu/~shiboxin/index.html">Boxin Shi</a>,
              <a href="https://nanduan.github.io/">Nan Duan</a>,
              <A href="">Ruofei Zhang</a>,
              <a href="">Liangjie Zhang</a>,
              <a href="">Weiwei Deng</a>,
              <a href="">Qi Zhang</a>
              <br>
              <em>ACM International Conference on Multimedia (ACM MM)</em>, 2022  
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548226">[paper]</a>   &nbsp; 
              <a href="data/Zhu_MM_2022.bib">[bibtex]</a>  &nbsp; 
              <a href="https://github.com/microsoft/CommercialAdsDataset/">[data]</a>
              <p>
                We propose a multi-modal relevance modeling approach for sponsored search, and boost the performance via contrastive learning that naturally extends the transformer encoder with the complementary multi-modal inputs. 
              </p>
            </td>
          </tr> 

          <!-- <tr onmouseout="face_stop()" onmouseover="face_start()" bgcolor="#ffffd0"> -->
           <tr onmouseout="face_stop()" onmouseover="face_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='face_image'><img src='images/face_resize.gif'></div>
                <img src='images/face_resize.png'>
              </div>
              <script type="text/javascript">
                function face_start() {
                  document.getElementById('face_image').style.opacity = "1";
                }

                function face_stop() {
                  document.getElementById('face_image').style.opacity = "0";
                }
                face_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Hybrid Face Reflectance, Illumination, and Shape from a Single Image</papertitle>
              </a>
              <br>
              <strong>Yongjie Zhu</strong>,
              <a href="http://lichenpro.com/">Chen Li</a>,
              <a href="">Si Li</a>,
              <a href="http://alumni.media.mit.edu/~shiboxin/index.html">Boxin Shi</a>,
              <a href="https://seng.ust.hk/about/people/faculty/yu-wing-tai">Yu-Wing Tai</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2021 &nbsp; 
              <br>
              <a href="https://ieeexplore.ieee.org/document/9431726">[paper]</a> &nbsp;  
              <a href="data/Zhu_TPAMI_2021.bib">[bibtex]</a> &nbsp; 
              <a href="https://youtu.be/USTbDW6MnEA">[video]</a> &nbsp; 
              <p></p>
              <p>
                We proposed a self-supervised deep learning framework that can estimate the hybrid reflection model and detailed normal of the human face. The proposed hybrid reflectance and illumination representation ensures the photo-realistic face reconstruction.
              </p>
            </td>
          </tr> 
 
          <tr onmouseout="solid_stop()" onmouseover="solid_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='solid_image'>
                  <img src='images/solid_after.png' width="160"></div>
                <img src='images/solid_before.png' width="160">
              </div>
              <script type="text/javascript">
                function solid_start() {
                  document.getElementById('solid_image').style.opacity = "1";
                }

                function solid_stop() {
                  document.getElementById('solid_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Spatially-Varying Outdoor Lighting Estimation from Intrinsics</papertitle>
              </a>
              <br>
              <strong>Yongjie Zhu</strong>,
              <a href="https://www.zhangyinda.com/">Yinda Zhang</a>,
              <a href="">Si Li</a>,
              <a href="http://alumni.media.mit.edu/~shiboxin/index.html">Boxin Shi</a>
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021 &nbsp; <font color="red"><b>(Oral Presentation)</b></font> 
              <br> 
              <a href="https://arxiv.org/abs/2104.04160">[arXiv]</a>   &nbsp; 
              <a href="data/Zhu_CVPR_2021.bib">[bibtex]</a> &nbsp; 
              <a href="https://youtu.be/O1M1k6JncoA">[video]</a> &nbsp; 
              <a href="https://drive.google.com/file/d/1_mzhuRQyv1jwTmTNh2UnuEr-0nMkZgPg/view?usp=sharing">[poster]</a> 
              <p></p>
              <p>
              Collecting high quality paired intrinsic and lighting data in a virtual city lets you train a model that estimates spatially-varying lighting from a single outdoor image.
              </p>
            </td>
          </tr> 
            
          <tr onmouseout="derender_stop()" onmouseover="derender_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='derender_image'>
                  <img src='images/derendernet_after.png' width="160"></div>
                <img src='images/derendernet_before.png' width="160">
              </div>
              <script type="text/javascript">
                function derender_start() {
                  document.getElementById('derender_image').style.opacity = "1";
                }

                function derender_stop() {
                  document.getElementById('derender_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>DeRenderNet: Intrinsic Image Decomposition of Urban Scenes with Shape-(In)dependent Shading Rendering</papertitle>
              </a>
              <br>
              <strong>Yongjie Zhu</strong>,
              <a href="https://scholar.google.com/citations?user=UBy9z8wAAAAJ&hl=zh-CN">Jiajun Tang</a>,
              <a href="">Si Li</a>,
              <a href="http://alumni.media.mit.edu/~shiboxin/index.html">Boxin Shi</a>
              <br>
              <em>International Conference on Computational Photography (ICCP)</em>, 2021  
              <br>
              <a href="https://arxiv.org/abs/2104.13602">[arXiv]</a>   &nbsp; 
              <a href="data/Zhu_ICCP_2021.bib">[bibtex]</a>
              <p>
                Decomposing a single RGB image into its reflectance, shading (caused by direct lighting), and shadow (caused by occlusion) images.
              </p>
            </td>
          </tr> 

        </tbody></table>


        <!-- <hr class="split"><br/> -->
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="97%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/cvpr_logo.png" alt="clean-usnob" width="100" height="40">
            </td>
            <td width="75%" valign="middle">
                Conference Reviewer, <a href="http://cvpr2021.thecvf.com/">CVPR, ICCV, ECCV</a>     
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/springer_logo.png" alt="clean-usnob" width="160" height="40">
            </td>
            <td width="75%" valign="middle">
              Journal Reviewer, <a href="https://www.springer.com/journal/11263/">IJCV, TPAMI</a>               
            </td>
          </tr>
          </tbody>
        </table>
       
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Current and Past Affiliations</heading>
            </td>
          </tr>
        </tbody></table>        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="10%" valign="middle">
              <a href="https://klingai.com/"><img src="media/kling_logo.png" width="78"></a>
            </td>            
            <td width="10%" valign="middle">
              <a href="https://www.alibabagroup.com/"><img src="media/logo-alibaba.png" width="78"></a>
            </td>

            <td width="10%" valign="middle">
              <a href="https://www.microsoft.com/"><img src="media/microsoft_logo1.png" width="75"></a>
            </td>

            <td width="10%" valign="middle">
              <a href="https://www.wechat.com/"><img src="media/wechat_logo.png" width="75"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://open.youtu.qq.com/"><img src="media/youtu_logo1.png" width="75"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://ci.idm.pku.edu.cn/"><img src="media/pku_logo.png" width="75"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.bupt.edu.cn/"><img src="media/bupt_logo.png" width="75"></a>
            </td>

      </td>      
    </tr>
    </table> 
    
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:10px;width:100%;vertical-align:middle">
                        <heading>Selected Honors</heading>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <ul>
                        <li><b>[2022]</b> Excellent graduation thesis of BUPT. </li>
                        <li><b>[2021]</b> CVPR outstanding reviewer. </li> 
                        <li><b>[2021]</b> China National Scholarship (国家奖学金). </li>
                        <li><b>[2020]</b> First Prize, Outstanding Student Scholarship of BUPT. </li>  
                        <li><b>[2019]</b> First Prize, Outstanding Student Scholarship of BUPT. </li> 
                        <li><b>[2018]</b> JJWorld (Beijing) Network Technology Scholarship. </li>
                    </ul>
                </tr>
                </tbody>
            </table>

    <hr class="split"><br/>
        
        <!-- <table width="97%" align="center" border="0" cellpadding="20"><tbody> -->
        <!--   <tr> -->
        <!--     <td style="padding:10px;width:25%;vertical-align:middle"> -->
        <!--       <body> -->
        <!--         <\!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=320&t=m&d=4G1UZkJRhwxNfETjjT_LOh2OxmkfIWo_k2YfL9xfO8A'></script> -\-> -->
        <!--         <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=320&t=tt&d=4G1UZkJRhwxNfETjjT_LOh2OxmkfIWo_k2YfL9xfO8A"></script> -->
        <!--       </body>               -->
        <!--     </td> -->
        <!--     <td width="25%" valign="middle"> -->
        <!--         Homepage of Yongjie Zhu [朱勇杰] <br> © Yongjie Zhu. All Rights Reserved.     -->
        <!--     </td> -->
 
        <!--   </tr> -->
        <!-- </tbody></table> -->
    
    <!--           CopyRight-->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td width=30% align="center">
            <script type="text/javascript" id="clustrmaps"
                    src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=4G1UZkJRhwxNfETjjT_LOh2OxmkfIWo_k2YfL9xfO8A"></script>
          </td>
          <td style="padding:10px">
            <br>
            <p style="text-align:right;">Homepage of Yongjie Zhu [朱勇杰] <br> © Yongjie Zhu. All Rights Reserved. </a></p>
          </td>
        </tr>
      </tbody>
    </table>

      </td>
    </tr>
  </table>
</body>

</html>
